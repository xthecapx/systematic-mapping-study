The systematic mapping review reveals that quantum computing technologies remain in their early developmental stages, evidenced by the limited literature addressing quality assurance in quantum computing. Current studies primarily focus on establishing theoretical quality attributes derived from classical software standards, with minimal exploration of quantum-specific metrics. Moreover, the documented quality attributes require validation in operational environments to assess their practical utility.

The validation of quality metrics requires carefully designed software experiments. These experiments should evaluate the feasibility of implementing literature-reported quality metrics within existing quantum infrastructure while executing controlled scenarios to identify potential software malfunctions.

The current quantum computing landscape lacks robust tools for system observability and monitoring. Part of the problem is caused by the prevalent model of Cloud Architecture adopted by quantum computers to facilitate access to them. Contemporary quantum services restrict direct hardware interaction, resulting in error correction mechanisms that rely, predominantly, on classical metrics. This limitation presents an opportunity for research aimed at enhancing developers' understanding of quantum software behavior within the existing infrastructure.

The analysis of hybrid systems reveals that cloud services represent the most viable solution for accessing quantum hardware, given the challenges of maintaining stable qubits and meeting specialized requirements such as ultra-low temperatures and resource constraints. However, this approach inherits traditional cloud computing challenges, including security risks, data governance issues, queue management and network dependencies, while simultaneously confronting quantum-specific challenges such as system availability, and exponential error propagation inherent to quantum phenomena.